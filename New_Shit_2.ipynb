{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matrix Factorization Code from Set 5\n",
    "\n",
    "Y_train = np.loadtxt('train.txt').astype(int)\n",
    "Y_test = np.loadtxt('test.txt').astype(int)\n",
    "mu = np.mean(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_U(Ui, Yij, Vj, reg, eta, Ai, Bj, bias):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Ui multiplied by eta.\n",
    "    \"\"\"\n",
    "    \n",
    "    if bias == False:\n",
    "        Ai = 0\n",
    "        Bj = 0\n",
    "    \n",
    "    \n",
    "    return eta*(reg*Ui - np.dot(Vj, (Yij - np.dot(Ui, Vj) + Ai + Bj)))\n",
    "\n",
    "def grad_V(Vj, Yij, Ui, reg, eta, Ai, Bj, bias):\n",
    "    \"\"\"\n",
    "    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n",
    "    Ui (the ith row of U), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Vj multiplied by eta.\n",
    "    \"\"\"\n",
    "    if bias == False:\n",
    "        Ai = 0\n",
    "        Bj = 0\n",
    "        \n",
    "    return eta*(reg*Vj - np.dot(Ui,(Yij - np.dot(Ui, Vj) + Ai + Bj)))\n",
    "\n",
    "def grad_A(Vj, Yij, Ui, reg, eta, Ai, Bj):\n",
    "\n",
    "    return eta*(reg*Ai - (Yij - (np.dot(Ui,Vj) + Ai + Bj)))\n",
    "\n",
    "def grad_B(Vj, Yij, Ui, reg, eta, Ai, Bj):\n",
    "\n",
    "    return eta*(reg*Bj - (Yij - (np.dot(Ui,Vj) + Ai + Bj)))\n",
    "\n",
    "def get_err(U, V, Y, A, B, bias, reg=0.0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j and\n",
    "    user/movie matrices U and V.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "    if bias: \n",
    "        first_term = 0.5*reg*(np.linalg.norm(U, ord='fro')**2 + np.linalg.norm(V, ord='fro')**2)\n",
    "        second_term = 0\n",
    "        for index in range(len(Y)):\n",
    "            (i, j, Yij) = Y[index]\n",
    "            second_term += (0.5 * (Yij - (np.dot(U[i - 1], V[j - 1])) + A[i-1] + B[j-1])**2)\n",
    "        return (first_term + second_term)/len(Y)\n",
    "    \n",
    "    else:\n",
    "        first_term = 0.5*reg*(np.linalg.norm(U, ord='fro')**2 + np.linalg.norm(V, ord='fro')**2)\n",
    "        second_term = 0\n",
    "        for index in range(len(Y)):\n",
    "            (i, j, Yij) = Y[index]\n",
    "            second_term += (0.5 * (Yij - (np.dot(U[i - 1], V[j - 1]))))\n",
    "        return (first_term + second_term)/len(Y)\n",
    "\n",
    "\n",
    "def get_err_bias(U, V, Y, A, B, bias, reg=0.0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j and\n",
    "    user/movie matrices U and V.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "\n",
    "    first_term = 0.5*reg*(np.linalg.norm(U, ord='fro')**2 + np.linalg.norm(V, ord='fro')**2)\n",
    "    second_term = 0\n",
    "    for index in range(len(Y)):\n",
    "        (i, j, Yij) = Y[index]\n",
    "        second_term += (0.5 * (Yij - (np.dot(U[i - 1], V[j - 1])) + A[i-1] + B[j-1])**2)\n",
    "    return (first_term + second_term)/len(Y)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(M, N, K, eta, reg, Y, bias, eps=0.0001, max_epochs=300):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n",
    "    by (UV^T)_ij.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n",
    "    of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    A = np.random.uniform(-0.5, 0.5, size = (M, ))\n",
    "    B = np.random.uniform(-0.5, 0.5, size = (N, ))\n",
    "        \n",
    "    U = np.random.uniform(-0.5, 0.5, size = (M, K))\n",
    "    V = np.random.uniform(-0.5, 0.5, size = (N, K))\n",
    "    \n",
    "    \n",
    "    MSE_before = get_err_bias(U, V, Y, A, B, bias, reg)\n",
    "    for epoch in range(max_epochs):\n",
    "        #print(epoch)\n",
    "        #print(MSE_before)\n",
    "        \n",
    "#         print(\"U:\", U[0,:])\n",
    "#         print(\"V:\", V[0,:])\n",
    "        np.random.shuffle(Y)\n",
    "        for index in range(len(Y)):\n",
    "            (i, j, Yij) = Y[index]\n",
    "            # Updates with gradient descent\n",
    "            u_grad = grad_U(U[i-1], Yij, V[j-1], reg, eta, A[i-1], B[i-1], bias)\n",
    "            v_grad = grad_V(V[j-1], Yij, U[i-1], reg, eta, A[i-1], B[i-1], bias)\n",
    "            a_grad = grad_A(V[j-1], Yij, U[i-1], reg, eta, A[i-1], B[j-1])\n",
    "            b_grad = grad_B(V[j-1], Yij, U[i-1], reg, eta, A[i-1], B[j-1])\n",
    "            A[i - 1] = A[i - 1] - a_grad\n",
    "            B[j - 1] = B[j - 1] - b_grad\n",
    "            U[i - 1] = U[i-1] - u_grad\n",
    "            V[j - 1] = V[j-1] - v_grad\n",
    "        #print(A[0:3], B[0:3])\n",
    "            \n",
    "        MSE_after = get_err(U, V, Y, A, B, bias, reg)\n",
    "        difference = MSE_before - MSE_after\n",
    "        if epoch == 0:\n",
    "            epoch1_MSE_diff = difference\n",
    "        if difference/epoch1_MSE_diff <= eps:\n",
    "            break\n",
    "        MSE_before = MSE_after\n",
    "\n",
    "    return U, V, MSE_before, A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with M = 943, N = 1682, k = 20, eta = 0.01, reg = 0.1\n"
     ]
    }
   ],
   "source": [
    "M = max(max(Y_train[:,0]), max(Y_test[:,0])).astype(int) # users\n",
    "N = max(max(Y_train[:,1]), max(Y_test[:,1])).astype(int) # movies\n",
    "\n",
    "\n",
    "\n",
    "#U,V, err = train_model(M, N, K, eta, reg, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Ks = [10,20,30,50,100]\n",
    "#Ks = [20]\n",
    "k = 20\n",
    "#regs = [10**-4, 10**-3, 10**-2, 10**-1, 1]\n",
    "regs = [0.1]\n",
    "etas = [0.01]\n",
    "#etas = [0.03, 0.01, 0.003, 0.001, 0.0003]  # learning rate\n",
    "#E_ins = []\n",
    "#E_outs = []\n",
    "\n",
    "bias = True\n",
    "# Use to compute Ein and Eout\n",
    "for reg in regs:\n",
    "    #E_ins_for_lambda = []\n",
    "    #E_outs_for_lambda = []\n",
    "\n",
    "    for eta in etas:\n",
    "        print(\"Training model with M = %s, N = %s, k = %s, eta = %s, reg = %s\"%(M, N, k, eta, reg))\n",
    "        U,V, e_in, A, B = train_model(M, N, k, eta, reg, Y_train, bias)\n",
    "        #E_ins_for_lambda.append(e_in)\n",
    "        eout = get_err(U, V, Y_test, A,B, bias)\n",
    "        #E_outs_for_lambda.append(eout)\n",
    "\n",
    "    #E_ins.append(E_ins_for_lambda)\n",
    "    #E_outs.append(E_outs_for_lambda)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.transpose(U)\n",
    "V = np.transpose(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 943) (20, 1682)\n",
      "[[3.76265369 2.85540112 2.97275445 ... 1.45437194 2.2609632  2.21659243]\n",
      " [4.20427707 3.14667248 3.33253308 ... 1.79001005 2.13654427 2.11530386]\n",
      " [3.07391739 2.33087449 2.60836748 ... 0.69084347 2.3555652  1.62504694]\n",
      " ...\n",
      " [4.59342565 3.4555351  3.02177759 ... 1.9699145  2.77301025 2.64543123]\n",
      " [4.65256205 3.75438518 3.31871789 ... 1.2550729  1.70802689 2.4157984 ]\n",
      " [3.90834106 3.2327043  3.57860273 ... 0.92784046 2.68902078 2.24031845]]\n",
      "-1.9371252061082849\n",
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "12845\n",
      "(943, 1682)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.shape(U), (np.shape(V)))\n",
    "\n",
    "\n",
    "\n",
    "res = np.matmul(np.transpose(U), V)\n",
    "\n",
    "print(res)\n",
    "print(np.min(res))\n",
    "\n",
    "print(res < 0)\n",
    "print(np.sum(res < 0))\n",
    "print(np.shape(res))\n",
    "\n",
    "#print(E_ins, E_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,S,B = np.linalg.svd(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65.85893806 16.57160389 12.62664983 12.0989987  11.40394184 11.19424999\n",
      " 10.85854527 10.37793831 10.2230872   9.89555689  9.78602619  9.68718686\n",
      "  9.46212238  9.34688252  9.21723115  9.14843481  8.88198852  8.75683582\n",
      "  8.62140335  8.42136336]\n",
      "(20, 20)\n",
      "(2, 943)\n",
      "(2, 1682)\n"
     ]
    }
   ],
   "source": [
    "print(S)\n",
    "print(np.shape(A))\n",
    "\n",
    "U_tilde = np.matmul(np.transpose(A)[0:2, :], U)\n",
    "V_tilde = np.matmul(np.transpose(A)[0:2, :], V)\n",
    "\n",
    "print(np.shape(U_tilde))\n",
    "print(np.shape(V_tilde))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.08673624  1.6354573   1.65036655 ...  0.71567517  0.97368686\n",
      "   1.13531816]\n",
      " [ 0.26729648  0.48966069  0.1647367  ... -0.44056387 -0.29530043\n",
      "  -0.60120002]]\n"
     ]
    }
   ],
   "source": [
    "print(V_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_love_movies = {}\n",
    "i_love_movies['The Shining'] = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
